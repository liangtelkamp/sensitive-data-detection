{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitive Data Detection Tutorial\n",
    "\n",
    "This notebook demonstrates the complete workflow of the sensitive data detection system, including:\n",
    "1. Data Processing with the data_processor module\n",
    "2. Language Detection with the language_detection module  \n",
    "3. PII and Non-PII Detection with the detect_reflect module\n",
    "4. Free Text Analysis with the free_text module\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's set up our environment and import necessary modules. Make sure you have created a `.env` file in the root directory with your API keys:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "HUGGINGFACE_API_KEY=your_huggingface_api_key_here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n",
      "Current working directory: /Users/liangtelkamp/Documents/GitHub/sensitive-data-detection/notebooks\n",
      "Project root added to path: /Users/liangtelkamp/Documents/GitHub/sensitive-data-detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add the project root to Python path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "print(f\"Project root added to path: {Path.cwd().parent}\")\n",
    "\n",
    "MODEL_NAME = \"gemma-3-4b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Data Processing\n",
    "\n",
    "Let's start by loading and processing our data using the data processor module. We'll use the dummy.csv file in the data directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data structure:\n",
      "\n",
      "Table: dummy\n",
      "Columns: ['report_date', 'location', 'access_level', 'service_coverage', 'population_total', 'facility_type', 'activity_type', 'region_name', 'access_constraints', 'group_vulnerability_map', 'incident_reports']\n",
      "Metadata: {'country': None, 'country_info': {'raw_country': None, 'standardized_name': None, 'alpha_2': None, 'alpha_3': None, 'standardization_confidence': 0.0, 'extraction_method': 'not_found', 'extracted_from_filename': False}, 'filename': 'dummy.csv', 'filepath': '../data/dummy.csv', 'table_name': 'dummy', 'file_extension': '.csv', 'file_size_bytes': 1402, 'processing_timestamp': '2025-06-17T16:29:03.687904', 'total_columns': 11, 'max_records_per_column': 20, 'column_names': ['report_date', 'location', 'access_level', 'service_coverage', 'population_total', 'facility_type', 'activity_type', 'region_name', 'access_constraints', 'group_vulnerability_map', 'incident_reports'], 'column_types': {'report_date': 'object', 'location': 'object', 'access_level': 'object', 'service_coverage': 'object', 'population_total': 'int64', 'facility_type': 'object', 'activity_type': 'object', 'region_name': 'object', 'access_constraints': 'object', 'group_vulnerability_map': 'object', 'incident_reports': 'int64'}}\n",
      "  report_date: ['2023-01-15', '2023-02-10', '2023-03-05', '2023-04-22', '2023-05-18']...\n",
      "  location: ['Niamey', 'Maradi', 'Zinder', 'TillabÃ©ri', 'Diffa']...\n",
      "  access_level: ['High', 'Medium', 'Low', 'High', 'Medium']...\n"
     ]
    }
   ],
   "source": [
    "from modules.data_processor import DataLoader\n",
    "\n",
    "# Initialize the data loader\n",
    "data_loader = DataLoader()\n",
    "\n",
    "# Load the dummy data\n",
    "data_path = \"../data/dummy.csv\"\n",
    "loaded_data = data_loader.load_data(data_path)\n",
    "\n",
    "# Display the structure of loaded data\n",
    "print(\"Loaded data structure:\")\n",
    "for table_name, table_data in loaded_data.items():\n",
    "    print(f\"\\nTable: {table_name}\")\n",
    "    print(f\"Columns: {list(table_data['columns'].keys())}\")\n",
    "    print(f\"Metadata: {table_data.get('metadata', {})}\")\n",
    "    \n",
    "    # Show sample data from first few columns\n",
    "    for i, (col_name, col_data) in enumerate(list(table_data['columns'].items())[:3]):\n",
    "        print(f\"  {col_name}: {col_data['records'][:5]}...\")  # First 5 records\n",
    "        if i >= 2:  # Limit to first 3 columns for display\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Language Detection\n",
    "\n",
    "Now let's detect the language of the content in our data using the language detection module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language for ../data/dummy.csv: en\n",
      "\n",
      "Language detection complete. Detected language: en\n"
     ]
    }
   ],
   "source": [
    "from modules.language_detection import LanguageDetector\n",
    "\n",
    "# Initialize the language detector\n",
    "lang_detector = LanguageDetector()\n",
    "\n",
    "# Detect language for the file\n",
    "try:\n",
    "    detected_language = lang_detector.detect_language(data_path)\n",
    "    print(f\"Detected language for {data_path}: {detected_language}\")\n",
    "except Exception as e:\n",
    "    print(f\"Language detection failed: {e}\")\n",
    "    detected_language = \"unknown\"\n",
    "\n",
    "# Store language info in our data structure\n",
    "for table_name, table_data in loaded_data.items():\n",
    "    if 'metadata' not in table_data:\n",
    "        table_data['metadata'] = {}\n",
    "    table_data['metadata']['detected_language'] = detected_language\n",
    "    \n",
    "print(f\"\\nLanguage detection complete. Detected language: {detected_language}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Setup LLM for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__init__() takes exactly one argument (the instance to initialize)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m MODEL_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma-3-4b-it\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Check if the model is ready\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is ready: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mis_ready()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/sensitive-data-detection/modules/llm_model/model.py:475\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muses_chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maya\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name\u001b[38;5;241m.\u001b[39mlower()\n",
      "\u001b[0;31mTypeError\u001b[0m: object.__init__() takes exactly one argument (the instance to initialize)"
     ]
    }
   ],
   "source": [
    "from modules.llm_model.model import Model\n",
    "\n",
    "MODEL_NAME = \"gemma-3-4b-it\"\n",
    "\n",
    "# Initialize the model\n",
    "model = Model(model_name=MODEL_NAME)\n",
    "\n",
    "# Check if the model is ready\n",
    "print(f\"Model {model.model_name} is ready: {model.is_ready()}\")\n",
    "\n",
    "# Get model components\n",
    "model_components = model.get_model_components()\n",
    "print(f\"Model components: {model_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. PII and Non-PII Detection with Detect-Reflect\n",
    "\n",
    "Now we'll use the detect_reflect module to identify both PII and non-PII sensitive information. This requires setting up a classifier that uses LLM models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized\n",
      "Initialized SensitivityClassifier with model: gpt-4o-mini\n",
      "Classifier setup complete!\n"
     ]
    }
   ],
   "source": [
    "from modules.detect_reflect import SensitivityClassifier, detect_and_reflect_pii, detect_non_pii\n",
    "\n",
    "\n",
    "# Initialize with real classifier\n",
    "sensitivity_classifier = SensitivityClassifier(\n",
    "    model_name=MODEL_NAME  # or \"gpt-4\" if you have access\n",
    ")\n",
    "print(f\"Initialized SensitivityClassifier with model: {sensitivity_classifier.model_name}\")\n",
    "\n",
    "print(\"Classifier setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 3.1 PII Detection and Reflection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PII detection and reflection...\n",
      "\n",
      "Processing table: dummy\n",
      "PII Detection Results:\n",
      "  report_date:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  location:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  access_level:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  service_coverage:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  population_total:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  facility_type:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  activity_type:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  region_name:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  access_constraints:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  group_vulnerability_map:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  incident_reports:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "\n",
      "PII detection and reflection complete!\n"
     ]
    }
   ],
   "source": [
    "# Perform PII detection and reflection\n",
    "print(\"Starting PII detection and reflection...\")\n",
    "\n",
    "for table_name, table_data in loaded_data.items():\n",
    "    print(f\"\\nProcessing table: {table_name}\")\n",
    "    \n",
    "    # Apply PII detection and reflection\n",
    "    table_data = detect_and_reflect_pii(table_data, sensitivity_classifier)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"PII Detection Results:\")\n",
    "    for col_name, col_data in table_data['columns'].items():\n",
    "        pii_detection_key = f\"pii_detection_{sensitivity_classifier.model_name}\"\n",
    "        pii_reflection_key = f\"pii_reflection_{sensitivity_classifier.model_name}\"\n",
    "        \n",
    "        pii_type = col_data.get(pii_detection_key, \"Not analyzed\")\n",
    "        sensitivity = col_data.get(pii_reflection_key, \"Not analyzed\")\n",
    "        \n",
    "        print(f\"  {col_name}:\")\n",
    "        print(f\"    PII Type: {pii_type}\")\n",
    "        print(f\"    Sensitivity: {sensitivity}\")\n",
    "\n",
    "print(\"\\nPII detection and reflection complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 3.2 Non-PII Sensitivity Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Non-PII sensitivity detection...\n",
      "\n",
      "Processing table: dummy\n",
      "\n",
      "### Instruction:\n",
      "You are a data governance assistant. Your task is to determine the overall sensitivity level of this table, based strictly on the provided Information Sharing Protocols (ISP) and the table's content and state which columns make the table sensitive.\n",
      "\n",
      "Follow these exact steps\n",
      "\n",
      "1. Analyze the table schema AND the records of the table.\n",
      "2. Use the ISP sensitivity levels: NON_SENSITIVE, MODERATE_SENSITIVE, HIGH_SENSITIVE, SEVERE_SENSITIVE. Only assign a sensitivity level if explicitly supported by ISP guidance, else assign NON_SENSITIVE.\n",
      "3. Analyze each rule of the ISP and identify whether the table falls into the category of the rule.\n",
      "4. If multiple sensitivity levels might apply, always choose the highest one explicitly mentioned in the ISP for the relevant data type. We rather be cautious than miss something.\n",
      "\n",
      "### Input:\n",
      "ISP Rules\n",
      "#### SEVERE_SENSITIVE: \n",
      "- Raw survey data, e.g. individual survey responses at HH-level data\n",
      "#### HIGH_SENSITIVE: \n",
      "- Aggregated survey results (e.g. aggregated to the household level and with additional disaggregation based on different indicators)\n",
      "#### MODERATE_SENSITIVE: \n",
      "- Aggregated survey results (e.g. aggregated to the district level)\n",
      "NON_SENSITIVE: \n",
      "- HNO and underlying national-level aggregate survey results\n",
      "\n",
      "Table:\n",
      "|    | report_date   | location   | access_level   | service_coverage   |   population_total | facility_type     | activity_type             | region_name   | access_constraints   | group_vulnerability_map      |   incident_reports |\n",
      "|---:|:--------------|:-----------|:---------------|:-------------------|-------------------:|:------------------|:--------------------------|:--------------|:---------------------|:-----------------------------|-------------------:|\n",
      "|  0 | 2023-01-15    | Niamey     | High           | 80%                |             120000 | Health Center     | Medical Aid               | Niamey        | Security checkpoints | High risk of food insecurity |                  5 |\n",
      "|  1 | 2023-02-10    | Maradi     | Medium         | 70%                |              95000 | Coordination Hub  | Logistical Support        | Maradi        | Restricted areas     | High malnutrition rates      |                  2 |\n",
      "|  2 | 2023-03-05    | Zinder     | Low            | 60%                |             110000 | Community Center  | Information Dissemination | Zinder        | Impassable roads     | Vulnerable child population  |                  3 |\n",
      "|  3 | 2023-04-22    | TillabÃ©ri  | High           | 85%                |             130000 | Field Office      | Resource Distribution     | TillabÃ©ri     | Military presence    | Severe drought impact        |                  7 |\n",
      "|  4 | 2023-05-18    | Diffa      | Medium         | 75%                |              80000 | Temporary Shelter | Refugee Assistance        | Diffa         | Curfews in effect    | Conflict displacement        |                  4 |\n",
      "\n",
      "### Response Format:\n",
      "- Sensitivity Classification: <ONE OF: NON_SENSITIVE / MODERATE_SENSITIVE / HIGH_SENSITIVE / SEVERE_SENSITIVE>\n",
      "- List with ONLY the columns that are sensitive or in combination with other columns are sensitive.\n",
      "- Cited ISP Rule(s): Quote the specific ISP rule(s) that directly support the classification. If none directly apply, explain briefly why it is NON_SENSITIVE.\n",
      "\n",
      "\n",
      "### Instruction:\n",
      "You are a data governance assistant. Your task is to determine the overall sensitivity level of this table, based strictly on the table's content and state which columns make the table sensitive.\n",
      "\n",
      "Follow these exact steps\n",
      "\n",
      "1. Analyze the table schema AND the records of the table.\n",
      "2. Identify ONLY the columns that are sensitive on their own OR that become sensitive in combination with others.\n",
      "3. Do look at the records of the table whether they contain sensitive information or not.\n",
      "\n",
      "NON_SENSITIVE: Information or data that, if disclosed or accessed without proper authorization, are unlikely to cause any harm or negative impacts to affected people and/or humanitarian actors.\n",
      "MODERATE_SENSITIVE: Information or data that, if disclosed or accessed without proper authorization, are likely to cause minor harm or negative impacts and/or be disadvantageous for affected people and/or humanitarian actors.\n",
      "HIGH_SENSITIVE: Information or data that, if disclosed or accessed without proper authorization, are likely to cause serious harm or negative impacts to affected people and/or humanitarian actors and/or damage to a response.\n",
      "SEVERE_SENSITIVE: Information or data that, if disclosed or accessed without proper authorization, are likely to cause severe harm or negative impacts and/or damage to affected people and/or humanitarian actors and/or impede the conduct of the work of a response.\n",
      "\n",
      "### Input:\n",
      "Table:\n",
      "|    | report_date   | location   | access_level   | service_coverage   |   population_total | facility_type     | activity_type             | region_name   | access_constraints   | group_vulnerability_map      |   incident_reports |\n",
      "|---:|:--------------|:-----------|:---------------|:-------------------|-------------------:|:------------------|:--------------------------|:--------------|:---------------------|:-----------------------------|-------------------:|\n",
      "|  0 | 2023-01-15    | Niamey     | High           | 80%                |             120000 | Health Center     | Medical Aid               | Niamey        | Security checkpoints | High risk of food insecurity |                  5 |\n",
      "|  1 | 2023-02-10    | Maradi     | Medium         | 70%                |              95000 | Coordination Hub  | Logistical Support        | Maradi        | Restricted areas     | High malnutrition rates      |                  2 |\n",
      "|  2 | 2023-03-05    | Zinder     | Low            | 60%                |             110000 | Community Center  | Information Dissemination | Zinder        | Impassable roads     | Vulnerable child population  |                  3 |\n",
      "|  3 | 2023-04-22    | TillabÃ©ri  | High           | 85%                |             130000 | Field Office      | Resource Distribution     | TillabÃ©ri     | Military presence    | Severe drought impact        |                  7 |\n",
      "|  4 | 2023-05-18    | Diffa      | Medium         | 75%                |              80000 | Temporary Shelter | Refugee Assistance        | Diffa         | Curfews in effect    | Conflict displacement        |                  4 |\n",
      "\n",
      "### Response Format:\n",
      "- Sensitivity Classification: <ONE OF: NON_SENSITIVE / MODERATE_SENSITIVE / HIGH_SENSITIVE / SEVERE_SENSITIVE>\n",
      "- Explain why you chose the sensitivity level.\n",
      "\n",
      "Non-PII Sensitivity Results:\n",
      "MODERATE_SENSITIVE\n",
      "- Sensitivity Classification: MODERATE_SENSITIVE\n",
      "- Sensitive Columns: location, service_coverage, population_total, facility_type, activity_type, region_name, access_constraints, group_vulnerability_map, incident_reports\n",
      "- Cited ISP Rule(s): \"Aggregated survey results (e.g. aggregated to the district level)\" applies here as the data can be interpreted as aggregated results at a level that could reflect vulnerabilities and service coverage in specific districts. The presence of population data and vulnerability indicators raises the sensitivity level to MODERATE_SENSITIVE. \n",
      "\n",
      "The table does not contain raw survey data at the household level (which would be SEVERE_SENSITIVE), nor does it contain national-level aggregate results (which would be NON_SENSITIVE). Thus, the highest applicable sensitivity level is MODERATE_SENSITIVE.\n",
      "\n",
      "  ISP Context Used: default\n",
      "\n",
      "Non-PII sensitivity detection complete!\n"
     ]
    }
   ],
   "source": [
    "# Perform Non-PII sensitivity detection\n",
    "print(\"Starting Non-PII sensitivity detection...\")\n",
    "\n",
    "for table_name, table_data in loaded_data.items():\n",
    "    print(f\"\\nProcessing table: {table_name}\")\n",
    "    \n",
    "    # Apply non-PII detection (column-level analysis)\n",
    "    table_data = detect_non_pii(table_data, sensitivity_classifier, table_name, method='table')\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Non-PII Sensitivity Results:\")\n",
    "    print(table_data['metadata'][f'non_pii_{MODEL_NAME}'])\n",
    "    print(table_data['metadata'][f'non_pii_{MODEL_NAME}_explanation'])\n",
    "    \n",
    "    # Check ISP used\n",
    "    isp_used = table_data.get('metadata', {}).get('isp_used', 'Unknown')\n",
    "    print(f\"\\n  ISP Context Used: {isp_used}\")\n",
    "\n",
    "print(\"\\nNon-PII sensitivity detection complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save results to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.detect_reflect.utils import save_json_data\n",
    "\n",
    "save_json_data(loaded_data, '../data/dummy_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we've demonstrated the complete workflow of the sensitive data detection system:\n",
    "\n",
    "1. **Data Processing**: We loaded and processed the dummy.csv file using the `DataLoader` class, which structured the data and extracted metadata including country information.\n",
    "\n",
    "2. **Language Detection**: We detected the primary language of the data using the `LanguageDetector` class.\n",
    "\n",
    "3. **PII and Non-PII Detection**: We used the `detect_reflect` module to:\n",
    "   - Identify PII entities in each column\n",
    "   - Reflect on the sensitivity level of identified PII\n",
    "   - Detect non-PII sensitive information using ISP (Information Sharing Policy) context\n",
    "\n",
    "4. **Free Text Analysis**: We identified which columns contain free text data and optionally analyzed them for PII content using the `FreeTextDetector`.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Modular Design**: Each step is handled by a specialized module\n",
    "- **API Key Management**: Uses environment variables for secure API key storage\n",
    "- **Mock Mode**: Provides demonstration capabilities even without API keys\n",
    "- **Comprehensive Analysis**: Covers both structured data analysis and free text detection\n",
    "- **ISP Context**: Uses location-based Information Sharing Policies for context-aware analysis\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Set up your OpenAI API key in the `.env` file for full functionality\n",
    "2. Try the workflow with your own data files\n",
    "3. Experiment with different ISP contexts for non-PII detection\n",
    "4. Customize the detection parameters for your specific use case\n",
    "\n",
    "The results from each step provide detailed insights for making informed decisions about data handling and privacy protection measures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Environment Setup Instructions\n",
    "\n",
    "To get full functionality from this tutorial, create a `.env` file in the project root directory with the following content:\n",
    "\n",
    "```bash\n",
    "# API Keys for Sensitive Data Detection Tutorial\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "HUGGINGFACE_API_KEY=your_huggingface_api_key_here\n",
    "\n",
    "```\n",
    "\n",
    "You can create this file by running the following command in your terminal from the project root:\n",
    "\n",
    "```bash\n",
    "cat > .env << 'EOF'\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "HUGGINGFACE_API_KEY=your_huggingface_api_key_here\n",
    "EOF\n",
    "```\n",
    "\n",
    "Then replace `your_openai_api_key_here` with your actual OpenAI API key.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
