{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitive Data Detection Tutorial\n",
    "\n",
    "This notebook demonstrates the complete workflow of the sensitive data detection system, including:\n",
    "1. Data Processing with the data_processor module\n",
    "2. Language Detection with the language_detection module  \n",
    "3. PII and Non-PII Detection with the detect_reflect module\n",
    "4. Free Text Analysis with the free_text module\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's set up our environment and import necessary modules. Make sure you have created a `.env` file in the root directory with your API keys:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "HUGGINGFACE_API_KEY=your_huggingface_api_key_here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n",
      "Current working directory: /Users/liangtelkamp/Documents/GitHub/sensitive-data-detection/notebooks\n",
      "Project root added to path: /Users/liangtelkamp/Documents/GitHub/sensitive-data-detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add the project root to Python path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "print(f\"Project root added to path: {Path.cwd().parent}\")\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Data Processing\n",
    "\n",
    "Let's start by loading and processing our data using the data processor module. We'll use the dummy.csv file in the data directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data structure:\n",
      "\n",
      "Table: dummy\n",
      "Columns: ['report_date', 'location', 'access_level', 'service_coverage', 'population_total', 'facility_type', 'activity_type', 'region_name', 'access_constraints', 'group_vulnerability_map', 'incident_reports']\n",
      "Metadata: {'country': None, 'country_info': {'raw_country': None, 'standardized_name': None, 'alpha_2': None, 'alpha_3': None, 'standardization_confidence': 0.0, 'extraction_method': 'not_found', 'extracted_from_filename': False}, 'filename': 'dummy.csv', 'filepath': '../data/dummy.csv', 'table_name': 'dummy', 'file_extension': '.csv', 'file_size_bytes': 1402, 'processing_timestamp': '2025-06-17T16:15:09.098812', 'total_columns': 11, 'max_records_per_column': 20, 'column_names': ['report_date', 'location', 'access_level', 'service_coverage', 'population_total', 'facility_type', 'activity_type', 'region_name', 'access_constraints', 'group_vulnerability_map', 'incident_reports'], 'column_types': {'report_date': 'object', 'location': 'object', 'access_level': 'object', 'service_coverage': 'object', 'population_total': 'int64', 'facility_type': 'object', 'activity_type': 'object', 'region_name': 'object', 'access_constraints': 'object', 'group_vulnerability_map': 'object', 'incident_reports': 'int64'}}\n",
      "  report_date: ['2023-01-15', '2023-02-10', '2023-03-05', '2023-04-22', '2023-05-18']...\n",
      "  location: ['Niamey', 'Maradi', 'Zinder', 'TillabÃ©ri', 'Diffa']...\n",
      "  access_level: ['High', 'Medium', 'Low', 'High', 'Medium']...\n"
     ]
    }
   ],
   "source": [
    "from modules.data_processor import DataLoader\n",
    "\n",
    "# Initialize the data loader\n",
    "data_loader = DataLoader()\n",
    "\n",
    "# Load the dummy data\n",
    "data_path = \"../data/dummy.csv\"\n",
    "loaded_data = data_loader.load_data(data_path)\n",
    "\n",
    "# Display the structure of loaded data\n",
    "print(\"Loaded data structure:\")\n",
    "for table_name, table_data in loaded_data.items():\n",
    "    print(f\"\\nTable: {table_name}\")\n",
    "    print(f\"Columns: {list(table_data['columns'].keys())}\")\n",
    "    print(f\"Metadata: {table_data.get('metadata', {})}\")\n",
    "    \n",
    "    # Show sample data from first few columns\n",
    "    for i, (col_name, col_data) in enumerate(list(table_data['columns'].items())[:3]):\n",
    "        print(f\"  {col_name}: {col_data['records'][:5]}...\")  # First 5 records\n",
    "        if i >= 2:  # Limit to first 3 columns for display\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Language Detection\n",
    "\n",
    "Now let's detect the language of the content in our data using the language detection module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language for ../data/dummy.csv: en\n",
      "\n",
      "Language detection complete. Detected language: en\n"
     ]
    }
   ],
   "source": [
    "from modules.language_detection import LanguageDetector\n",
    "\n",
    "# Initialize the language detector\n",
    "lang_detector = LanguageDetector()\n",
    "\n",
    "# Detect language for the file\n",
    "try:\n",
    "    detected_language = lang_detector.detect_language(data_path)\n",
    "    print(f\"Detected language for {data_path}: {detected_language}\")\n",
    "except Exception as e:\n",
    "    print(f\"Language detection failed: {e}\")\n",
    "    detected_language = \"unknown\"\n",
    "\n",
    "# Store language info in our data structure\n",
    "for table_name, table_data in loaded_data.items():\n",
    "    if 'metadata' not in table_data:\n",
    "        table_data['metadata'] = {}\n",
    "    table_data['metadata']['detected_language'] = detected_language\n",
    "    \n",
    "print(f\"\\nLanguage detection complete. Detected language: {detected_language}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Setup LLM for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized\n",
      "Model gpt-4o-mini is ready: True\n",
      "Model components: (None, None, <openai.OpenAI object at 0x134049750>, 'openai')\n"
     ]
    }
   ],
   "source": [
    "from modules.llm_model.model import Model\n",
    "\n",
    "# Initialize the model\n",
    "model = Model(model_name=MODEL_NAME)\n",
    "\n",
    "# Check if the model is ready\n",
    "print(f\"Model {model.model_name} is ready: {model.is_ready()}\")\n",
    "\n",
    "# Get model components\n",
    "model_components = model.get_model_components()\n",
    "print(f\"Model components: {model_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. PII and Non-PII Detection with Detect-Reflect\n",
    "\n",
    "Now we'll use the detect_reflect module to identify both PII and non-PII sensitive information. This requires setting up a classifier that uses LLM models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized\n",
      "Initialized SensitivityClassifier with model: gpt-4o-mini\n",
      "Classifier setup complete!\n"
     ]
    }
   ],
   "source": [
    "from modules.detect_reflect import SensitivityClassifier, detect_and_reflect_pii, detect_non_pii\n",
    "\n",
    "\n",
    "# Initialize with real classifier\n",
    "sensitivity_classifier = SensitivityClassifier(\n",
    "    model_name=MODEL_NAME  # or \"gpt-4\" if you have access\n",
    ")\n",
    "print(f\"Initialized SensitivityClassifier with model: {sensitivity_classifier.model_name}\")\n",
    "\n",
    "print(\"Classifier setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 3.1 PII Detection and Reflection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PII detection and reflection...\n",
      "\n",
      "Processing table: dummy\n",
      "PII Detection Results:\n",
      "  report_date:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  location:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  access_level:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  service_coverage:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  population_total:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  facility_type:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  activity_type:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  region_name:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  access_constraints:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  group_vulnerability_map:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "  incident_reports:\n",
      "    PII Type: None\n",
      "    Sensitivity: NON_SENSITIVE\n",
      "\n",
      "PII detection and reflection complete!\n"
     ]
    }
   ],
   "source": [
    "# Perform PII detection and reflection\n",
    "print(\"Starting PII detection and reflection...\")\n",
    "\n",
    "for table_name, table_data in loaded_data.items():\n",
    "    print(f\"\\nProcessing table: {table_name}\")\n",
    "    \n",
    "    # Apply PII detection and reflection\n",
    "    table_data = detect_and_reflect_pii(table_data, sensitivity_classifier)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"PII Detection Results:\")\n",
    "    for col_name, col_data in table_data['columns'].items():\n",
    "        pii_detection_key = f\"pii_detection_{sensitivity_classifier.model_name}\"\n",
    "        pii_reflection_key = f\"pii_reflection_{sensitivity_classifier.model_name}\"\n",
    "        \n",
    "        pii_type = col_data.get(pii_detection_key, \"Not analyzed\")\n",
    "        sensitivity = col_data.get(pii_reflection_key, \"Not analyzed\")\n",
    "        \n",
    "        print(f\"  {col_name}:\")\n",
    "        print(f\"    PII Type: {pii_type}\")\n",
    "        print(f\"    Sensitivity: {sensitivity}\")\n",
    "\n",
    "print(\"\\nPII detection and reflection complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 3.2 Non-PII Sensitivity Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Non-PII sensitivity detection...\n",
      "\n",
      "Processing table: dummy\n",
      "Non-PII Sensitivity Results:\n",
      "MODERATE_SENSITIVE\n",
      "- Sensitivity Classification: MODERATE_SENSITIVE\n",
      "- Sensitive Columns: location, access_level, service_coverage, population_total, facility_type, activity_type, region_name, access_constraints, group_vulnerability_map, incident_reports\n",
      "- Cited ISP Rule(s): The table contains aggregated survey results that are potentially sensitive at the district level, as indicated by the presence of location, population data, and vulnerability assessments. This aligns with the MODERATE_SENSITIVE classification under the ISP, specifically the rule: \"Aggregated survey results (e.g. aggregated to the district level).\" The data includes indicators of service coverage and group vulnerabilities, which could reveal sensitive information about specific populations in the districts.\n",
      "\n",
      "  ISP Context Used: default\n",
      "\n",
      "Non-PII sensitivity detection complete!\n"
     ]
    }
   ],
   "source": [
    "# Perform Non-PII sensitivity detection\n",
    "print(\"Starting Non-PII sensitivity detection...\")\n",
    "\n",
    "for table_name, table_data in loaded_data.items():\n",
    "    print(f\"\\nProcessing table: {table_name}\")\n",
    "    \n",
    "    # Apply non-PII detection (column-level analysis)\n",
    "    table_data = detect_non_pii(table_data, sensitivity_classifier, table_name, method='table')\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Non-PII Sensitivity Results:\")\n",
    "    print(table_data['metadata'][f'non_pii_{MODEL_NAME}'])\n",
    "    print(table_data['metadata'][f'non_pii_{MODEL_NAME}_explanation'])\n",
    "    \n",
    "    # Check ISP used\n",
    "    isp_used = table_data.get('metadata', {}).get('isp_used', 'Unknown')\n",
    "    print(f\"\\n  ISP Context Used: {isp_used}\")\n",
    "\n",
    "print(\"\\nNon-PII sensitivity detection complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save results to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.detect_reflect.utils import save_json_data\n",
    "\n",
    "save_json_data(loaded_data, '../data/dummy_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we've demonstrated the complete workflow of the sensitive data detection system:\n",
    "\n",
    "1. **Data Processing**: We loaded and processed the dummy.csv file using the `DataLoader` class, which structured the data and extracted metadata including country information.\n",
    "\n",
    "2. **Language Detection**: We detected the primary language of the data using the `LanguageDetector` class.\n",
    "\n",
    "3. **PII and Non-PII Detection**: We used the `detect_reflect` module to:\n",
    "   - Identify PII entities in each column\n",
    "   - Reflect on the sensitivity level of identified PII\n",
    "   - Detect non-PII sensitive information using ISP (Information Sharing Policy) context\n",
    "\n",
    "4. **Free Text Analysis**: We identified which columns contain free text data and optionally analyzed them for PII content using the `FreeTextDetector`.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Modular Design**: Each step is handled by a specialized module\n",
    "- **API Key Management**: Uses environment variables for secure API key storage\n",
    "- **Mock Mode**: Provides demonstration capabilities even without API keys\n",
    "- **Comprehensive Analysis**: Covers both structured data analysis and free text detection\n",
    "- **ISP Context**: Uses location-based Information Sharing Policies for context-aware analysis\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Set up your OpenAI API key in the `.env` file for full functionality\n",
    "2. Try the workflow with your own data files\n",
    "3. Experiment with different ISP contexts for non-PII detection\n",
    "4. Customize the detection parameters for your specific use case\n",
    "\n",
    "The results from each step provide detailed insights for making informed decisions about data handling and privacy protection measures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Environment Setup Instructions\n",
    "\n",
    "To get full functionality from this tutorial, create a `.env` file in the project root directory with the following content:\n",
    "\n",
    "```bash\n",
    "# API Keys for Sensitive Data Detection Tutorial\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "HUGGINGFACE_API_KEY=your_huggingface_api_key_here\n",
    "\n",
    "```\n",
    "\n",
    "You can create this file by running the following command in your terminal from the project root:\n",
    "\n",
    "```bash\n",
    "cat > .env << 'EOF'\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "HUGGINGFACE_API_KEY=your_huggingface_api_key_here\n",
    "EOF\n",
    "```\n",
    "\n",
    "Then replace `your_openai_api_key_here` with your actual OpenAI API key.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
